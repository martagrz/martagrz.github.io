---
title: Preference Extraction and Reward Learning
summary: Uncovering reward (utility) functions in economic settings. 
tags:
  - Microeconomics 
  - Computational Economics 
  - Machine Learning 
  - Inverse Reinforcement Learning
date: '2024-05-18T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart

#links:
#  - icon: twitter
#    icon_pack: fab
#    name: Follow
#    url: https://twitter.com/georgecushen
#url_code: ''
#url_pdf: ''
#url_slides: ''
#url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
#slides: example
---

Uncovering reward (utility) functions in economic settings. Determining preferences and utility is a foundational challenge in economics. They are central in determining behaviour through the utility-maximising decision-making process. However, preferences and utilities are not observable and may not even be known to the individual making the choice; only the outcome is observed. As such, this project aims to develop general algorithms and approaches to uncovering preferences and utility functions from observational data.

The first setting to consider is the consumer problem, where the outcome of utility-maximising is in the form of demand. Without the ability to observe the decision-making mechanism, demand estimation becomes a challenging task and current methods fall short due to lack of scalability or ability to identify causal effects. To address the shortcomings of existing methods, we present an algorithmic framework for uncovering a utility function based on observational consumption data.
